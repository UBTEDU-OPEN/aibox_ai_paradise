[common_string]
ubt_loading_tip1 = 神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和一个隐藏层。
ubt_loading_tip2 = 人工智能诞生于 20 世纪 50 年代。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。
ubt_loading_tip3 = 1983年，John Hopfield利用神经网络，在旅行社问题这个NP完全问题上获得了当时最好的结果，引起轰动。
ubt_loading_tip4 = 2012 年在ImageNet图像分类挑战赛中，深度学习模型AlexNet一举夺冠，从此深度卷积神经网络（CNN）成为了所有计算机视觉任务的首选算法。
ubt_loading_tip5 = 在神经网络中，我们通常需要随机初始化模型参数，然后通过反向传播（Back Propagation）算法来学习模型参数。
ubt_loading_tip6 = 深度学习对比传统方法来说，最大的优势是特征的自动提取。因此，其对数据集的表达更高效和准确，所提取的抽象特征鲁棒性更强，泛化能力更好。
ubt_loading_tip7 = 为了提高模型的泛化能力，通常会对训练数据做数据增强（data augmentation），如翻转、旋转等。
ubt_loading_tip8 = 深度学习常用框架有Pytorch、Tensorflow、Keras和Caffe等等。对于新手来说，Keras可能是你的最好选择，因为它封装的API是更为抽象、高级的。
ubt_loading_tip9 = RNN(循环神经网络)是一类用于处理序列数据的神经网络模型，在自然语言处理领域有较多应用，也被用于各类时间序列预报。
ubt_loading_tip10 = LSTM（长短时记忆网络）可以控制何时让输入进入神经元，何时记住之前时序中学到的东西，以及何时让输出传递到下一个时间戳。
ubt_loading_tip11 = 迁移学习是一种机器学习的方法，指的是一个预训练的模型被重新用在另一个任务中，可以让你对第二个任务建模的进展变快或提高它的性能。
ubt_loading_tip12 = Python 是一种解释型、面向对象、动态数据类型的高级程序设计语言，广泛用于当下火热的机器学习、大数据分析、网络爬虫中。
ubt_loading_tip13 = 2020 年 1 月 1 日， Python 2 停止更新。因此，建议大家尽量都使用Python 3来编写程序。
ubt_loading_tip14 = 在 Python 2.x 中 ，使用 / 除法，整数相除的结果是一个整数，把小数部分完全忽略掉。例如5/3的结果是1。
ubt_loading_tip15 = 在 Python 3.x 中 ，使用/ 除法，不同于Python 2.x中，对于整数之间的相除，结果会是浮点数。例如5/3的结果是1.66666666667。
ubt_loading_tip16 = 许多现代神经网络的实现基于GPU，GPU最初是为图形应用而开发的专用硬件组件。因此神经网络受益于游戏产业的发展。
ubt_loading_tip17 = CPU就像1位大学生，10分钟能做一道曲面积分，但是他10分钟内做不完1000道百位数加法。而GPU就像100个小学生，可以一起10分钟完成1000道百位数加法题。
ubt_loading_tip18 = 常规的人脸识别流程是：人脸检测-人脸对齐-提取人脸特征-人脸特征比对。
ubt_loading_tip19 = 图片分类任务就是判断图片中的对象是不是物体A。而目标检测任务则意味着，我们不仅要用算法判断图片中有没有物体A，还要在图片中找出它在哪，即它的包围框。
ubt_loading_tip20 = 语义分割的任务是通过一定的方法将图像分割成具有一定语义含义的区域块，并识别出每个区域块的语义类别，实现从底层到高层的语义推理过程，最终得到一幅具有逐像素语义标注的分割图像。
ubt_loading_tip21 = 目前语义分割的应用领域主要有：地理信息系统、无人车驾驶、医疗影像分析和机器人等领域。
ubt_loading_tip22 = 过拟合与欠拟合的区别在于，欠拟合在训练集和测试集上的性能都较差，而过拟合往往能较好地学习训练集数据的性质，而在测试集上的性能较差。
ubt_loading_tip23 = 图灵测试指的是电脑在5分钟内回答由人类测试者提出的一系列问题，如果其超过30%的回答让测试者误认为是人类所答，则电脑通过测试。
ubt_loading_tip24 = 生成对抗网络（Generative Adversarial Network，简称GAN）是非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。
ubt_loading_tip25 = 2006年，被称为深度学习元年。在这一年，杰弗里 ·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。杰弗里 ·辛顿也因此被称为深度学习之父。
ubt_loading_tip26 = 2019年3月27日，ACM（美国计算机协会）宣布，有“深度学习三巨头”之称Yoshua Bengio、Yann LeCun、Geoffrey Hinton共同获得了2018年的图灵奖。
ubt_loading_tip27 = 2018年由google推出的BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。
ubt_loading_tip28 = 1969年，单层神经网络被指出无法解决非线性问题，而当时多层网络的训练尚看不到希望。这个论断导致神经网络的研究进入冰河期。
ubt_loading_tip29 = 深度学习能于2010再度崛起的原因主要有：1.硬件上的提升，特别是GPU的发展，使得矩阵运算速度大幅提升。2.大规模数据集的出现。3.训练算法的改进。
ubt_loading_tip30 = 深度学习做识别，精度高，特征提取的过程是自学习的，泛化能力比较强，但是需要大量的标注数据去训练，对硬件要求高。

